{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ6SNYq_tVVC",
    "tags": []
   },
   "source": [
    "# IMDB 文本分类和模型比较\n",
    "\n",
    "**摘要：**我们使用 IMDB 数据集进行文本分类。在文本预处理阶段，我们尝试使用词编码和词向量的方式，在训练阶段，我们构建了 DNN、LSTM、BERT 等多个深度学习模型进行训练，并进行了模型比较，最高达到了 99% 的准确率。最后，为了进一步实现在超大文本集上进行训练，我们使用基于 Spark 的分布式算法在集群服务器上进行训练测试。\n",
    "\n",
    "\n",
    "\n",
    "| 模型         | 计算配置 | 用时 | 准确率 | 可拓展性 |\n",
    "| ----------- | ----------- | ------ | ------ | --- |\n",
    "| tokenize + DNN  |阿里云服务器 Xeon 8 核 CPU 32G 内存| 10  分钟 | 60% | 低-单机 |\n",
    "| Word2Vec + LSTM  |阿里云服务器 Xeon 8 核 CPU 32G 内存| 2 小时| 80% | 低-单机 |\n",
    "| bert - 小型       |阿里云服务器 Xeon 8 核 CPU 32G 内存| 1  小时 | 90% | 低-单机 |\n",
    "| bert - AL        |阿里云服务器 Xeon 8 核 CPU 32G 内存| 1.5  小时 | 90% | 低-单机 |\n",
    "| bert - 标准      |阿里云服务器 Xeon 8 核 CPU 32G 内存| 3  小时 | 92% | 低-单机 |\n",
    "| spark            |中央财经大学大数据高性能分布式集群    | --  分钟 | -% | 高-集群 |\n",
    "\n",
    "> 分布式模型在该小型数据集上没有优势，进行此项的意义在于对大型文本数据集可拓展性的技术储备，仅有在文本量级超过单机可承载上限时，分布式计算才具备意义\n",
    "\n",
    "> 注意：请勿在低配置计算机上运行该笔记本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "source": [
    "## 环境安装及设置\n",
    "\n",
    "请预先安装以下模块：\n",
    "\n",
    "- tensorflow (2.0以上版本）\n",
    "- tensorflow-text（用于文字预处理）\n",
    "- tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_XgTpm9ZxoN9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 17:46:34.731743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-14 17:46:34.731798: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydot\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "from official.nlp import optimization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "### 标签处理\n",
    "\n",
    "我们将分类列由类别名称转为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6IwI_2bcIeX8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------> 加载数据\n",
    "dat = pd.read_csv('./data/IMDB.csv')\n",
    "\n",
    "# ------------------> 标签处理\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(dat['sentiment'])\n",
    "y = encoder.transform(dat['sentiment'])\n",
    "text_labels = encoder.classes_\n",
    "text_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分\n",
    "\n",
    "划分 20% 的测试集，再从训练集中划分 20% 的验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dat['review'], y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集格式转换\n",
    "\n",
    "我们将数据集转换为 `tf.data.Dataset` 的格式，使用自动的缓冲区大小，使用 32 的 `batch size`\n",
    "\n",
    "batch size 的大小与模型的收敛速度和随机梯度噪音有关。当 batch size 过小时，在一定的迭代次数下，模型来不及收敛。当 batch size 过大时，一方面容易出现内存紧缺，另一方面模型的泛化能力会变差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 17:46:38.405180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-14 17:46:38.405224: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-14 17:46:38.405249: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nlp): /proc/driver/nvidia/version does not exist\n",
      "2022-01-14 17:46:38.405617: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train))\n",
    "raw_val_ds = tf.data.Dataset.from_tensor_slices((X_val.values, y_val))\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices((X_test.values, y_test))\n",
    "\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGm10A5HRGXp"
   },
   "source": [
    "我们对数据集进行预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JuxDkcvVIoev",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b\"This is like something I have NEVER seen before. It had me cracking up the whole time I don't think there was one scene that I didn't laugh through. It is about a girl from the country in South who goes off to a big town for college. At the school she befriends the RA across the hall. When she realizes that he has no family to go to for Thanksgiving she invites him to come home with her. Rabecca and her family and her serious boyfriend all go out to dinner one night and Becca realizes what her boyfriend is about to do...Propose. She urges Cral to do something so he stands up and shouts something like... Sorry mate but you are too late I already asked Becca to marry me a couple of weeks ago back at the school and she said yes. That all turns into Chaos. Please watch this classic it is totally worth it... I swear.\"\n",
      "Label : 1 (positive)\n",
      "Review: b'I managed to see this at the New York International Film Festival in November 2005 with my boyfriend. We were both quite impressed with the complexity of the plot and found it to be emotionally moving. It was very well directed with strong imagery. The visual effects were amazing - especially for a short. It had an original fantasy approach to a very real and serious topic: This film is about a young girl who is visited by a demon offering to help her situation with her abusive father. There is also a surprise twist at the end which caught me off guard. This leans towards the Gothic feel. I would love to see this as a full feature film. -- Carrie'\n",
      "Label : 1 (positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 17:46:40.406811: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(2):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({text_labels[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize + DNN + 防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (32000, 1000)\n",
      "x_test shape: (10000, 1000)\n",
      "y_train shape: (32000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "max_words = 1000\n",
    "tokenize = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, \n",
    "                                              char_level=False)\n",
    "tokenize.fit_on_texts(X_train) # fit tokenizer to our training text data\n",
    "x_train_token = tokenize.texts_to_matrix(X_train)\n",
    "x_test_token = tokenize.texts_to_matrix(X_test)\n",
    "y_train_token = y_train\n",
    "y_test_token = y_test\n",
    "print('x_train shape:', x_train_token.shape)\n",
    "print('x_test shape:', x_test_token.shape)\n",
    "print('y_train shape:', y_train_token.shape)\n",
    "print('y_test shape:', y_test_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过多次调参，我们发现增大全连接层输出的维度能有效提升准确率。\n",
    "\n",
    "为了减轻训练过程中的过拟合现象，我们使用 dropout 方法来增强神经元的协同适应能力。我们在输入层和中间层分别加入 50% dropout ，即每个神经元有 50% 的概率被随机剔除。由于输出层是我们所需的结果，不使用 dropout 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 512)               512512    \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 775,681\n",
      "Trainable params: 775,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100 \n",
    "drop_ratio = 0.5\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, input_shape=(max_words,)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(drop_ratio))\n",
    "model.add(tf.keras.layers.Dense(512))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(drop_ratio))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.2763 - accuracy: 0.7858 - val_loss: 2.5910 - val_accuracy: 0.8303\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 2.9408 - accuracy: 0.8068 - val_loss: 2.9467 - val_accuracy: 0.8078\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.0207 - accuracy: 0.8022 - val_loss: 3.1660 - val_accuracy: 0.7937\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 2.9724 - accuracy: 0.8055 - val_loss: 2.7487 - val_accuracy: 0.8203\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 2.9257 - accuracy: 0.8082 - val_loss: 2.7482 - val_accuracy: 0.8197\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 2.8469 - accuracy: 0.8140 - val_loss: 2.6141 - val_accuracy: 0.8281\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.8244 - accuracy: 0.7502 - val_loss: 2.7282 - val_accuracy: 0.8216\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.6653 - accuracy: 0.7597 - val_loss: 3.2134 - val_accuracy: 0.7887\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.1868 - accuracy: 0.7918 - val_loss: 2.6701 - val_accuracy: 0.8238\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.4140 - accuracy: 0.7774 - val_loss: 2.6777 - val_accuracy: 0.8250\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 3.6333 - accuracy: 0.7621 - val_loss: 3.6215 - val_accuracy: 0.7616\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "history = model.fit(x_train_token, y_train_token,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback],\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 3.7708 - accuracy: 0.7527\n",
      "Test loss: 3.770777702331543\n",
      "Test accuracy: 0.7526999711990356\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_token, y_test_token,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理\n",
    "\n",
    "我们建立词字典进行词编码，字典的大小限制在 1000 词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码的长度由所有文本中最长的文本决定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder(text_batch)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印示例："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", text_batch[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 第一层为 `Embedding` 层，我们使用 `word2vec` 方法将单词编码转换为词向量。这些词向量经过训练，对于意思相近的词，其向量夹角小。\n",
    "2. 第二层使用双向的长短期记忆层。长短期记忆网络层是一种特殊的循环神经网络层，它能够减轻长序列训练过程中的梯度消失和梯度爆炸问题，适合此处词向量长度较长的情况。它遍历序列中的每个元素作为输入，按照时间顺序传递输出。由于我们使用双向结构，最终结果由输入的前向和后向传递共同决定，这使得最前端的输入不必通过漫长的处理步数才能影响到最终结果，有效的提高了训练在文本中的均匀度。\n",
    "3. 第三层为全连接层，由于在多层神经网络中梯度容易在深层网络中变得极小，使得参数无法正常更新，所以我们使用 `RELU` 作为激活函数解决梯度消失问题。\n",
    "4. 第四层为输出维度为 5 的输出层，为了得到多分类的概率值，使用 `softmax` 函数将输出值压缩至 0 - 1 的范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=10,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "## BERT 简介\n",
    "\n",
    "BERT 是一系列双向文字编码转换模型的总称，用来结合上下文语义计算每个词的词向量，在自然语言处理中被广泛使用。\n",
    "\n",
    "我们使用了前人在超大型语料库上训练的已有基础 BERT 模型，通过迁移学习的方式在我们的 BBC 文本数据集上进行微调。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "## 加载预训练 BERT 模型\n",
    "\n",
    "我们首先使用了一个参数量较少的 small-BERT 模型用于测试，在通过测试后，为了进一步提升模型的准确度，我们使用 al-BERT 进行正式训练。\n",
    "\n",
    "> 在该笔记本中，我们仅展示 small-BERT 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = 'https://storage.googleapis.com/tfhub-modules/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1.tar.gz'\n",
    "tfhub_handle_preprocess = 'https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_preprocess/3.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "## 预处理模型\n",
    "\n",
    "在 BERT 的输入层，对于原始的文字输入，我们需要将其转换成为数值编码。每一个 BERT 模型都有其严格对应的预处理模型来提升转换效果。\n",
    "\n",
    "我们展示该预处理模型的输出结果，可以看到改预处理模型将输入的向量设为 128 的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9-zCzJpnuwS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "text_test = ['The first sentence. The second sentence.']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "## BERT 模型\n",
    "\n",
    "在进行迁移学习之前，我们先看预训练 BERT 模型的输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OoF9mebuSZc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## 迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aksj743St9ga",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1)(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs4yhFraBuGQ"
   },
   "source": [
    "在开始训练之前，我们测试模型搭建过程是否有误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGMF8AZcB2Zy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EmzyHZXKIpm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302",
    "tags": []
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### 损失函数\n",
    "\n",
    "我们使用交叉熵作为我们的损失函数：\n",
    "\n",
    "$$ -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}) $$\n",
    "\n",
    "其中：\n",
    "\n",
    "- M 是分类数\n",
    "- y 是标签 c 在观测 o 下是否分类正确的 0/1 变量\n",
    "- p 是预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWPOZE-L3AgE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "### 学习率\n",
    "\n",
    "由于神经网络刚开始训练时非常不稳定，因此刚开始的学习率应当设置得很低很低，这样可以保证网络能够具有良好的收敛性。但是较低的学习率会使得训练过程变得非常缓慢，因此这里采用从较低学习率逐渐增大至较高学习率的方式实现网络训练前 10% 次迭代的“热身”阶段。一直使用较高学习率是不合适的，因为它会使得权重的梯度一直来回震荡，很难使训练的损失值达到全局最低谷。因此在 warm-up 结束后，我们使用线性减小的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9eP2y9dbw32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "### 优化器\n",
    "\n",
    "在迁移学习时，我们选取的优化器与 BERT 在预训练时的 `Adamw` 优化器保持一致。\n",
    "\n",
    "<img src=\"./figure/Adamw.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Adam 的超收敛性质使其在训练学习率高的神经网络时可以达到节省迭代次数的效果。只要调整得当，Adam 在实践上都能达到 SGD+Momentum 的高准确率，而且速度更快。在几年前人们普遍认为 Adam 的泛化性能不如 SGD+Momentum，然而今年论文表明这通常是由于所选择的超参数不正确导致，通常来说 Adam 需要的正则化比 SGD 更多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9eP2y9dbw32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtfDFAnN_Neu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "print('----- 训练开始 -----')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)\n",
    "print('----- 训练完成 -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiythcODf0xo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### 模型评价\n",
    "\n",
    "我们在测试集上计算分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slqB-urBV9sP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=22)\n",
    "    plt.yticks(tick_marks, classes, fontsize=22)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=25)\n",
    "    plt.xlabel('Predicted label', fontsize=25)\n",
    "    \n",
    "predict_probability = classifier_model.predict(test_ds)\n",
    "prediction = [np.argmax(i) for i in predict_probability]\n",
    "cnf_matrix = confusion_matrix(y_test.tolist(), prediction)\n",
    "plt.figure(figsize=(24,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=text_labels, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## 模型应用\n",
    "\n",
    "我们将经过训练完成的模型保存，方便调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShcvqJAgVera",
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_model.save('./model/IMDB_bert', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用模型对输入的文本进行分类。\n",
    "\n",
    "我们输入一则测试新闻文本：“这部电影很差劲”，该文本被模型分类为消极，符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('./model/IMDB_bert')\n",
    "query = ['This movie is so bad']\n",
    "result = tf.sigmoid(model(tf.constant(query)))\n",
    "print('----- 评论积极的概率 -----')\n",
    "dict(zip(text_labels, result.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 通过 YARN 资源调度系统提交到作业队列： `spark-submit --master yarn`\n",
    "* 由于在 UDF（用户自定义）函数中使用了第三方包，需要将其发送至集群中的每个计算节点 `--py-files gensim.zip`\n",
    "* 队列计算完成后将结果重定向输出 `> output.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark-submit --master yarn code.py --py-files gensim.zip > output.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ----------------------------> 特征工程方法选项\n",
    "processType = 'word2vec'\n",
    "#  processType = 'vectorize-idf'\n",
    "#  processType = 'tf-idf'\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 文本清洁选项\n",
    "cleaning = False\n",
    "#  cleaning = True # 需要上传第三方包 gensim\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 包及环境启动\n",
    "import pandas as pd\n",
    "import pyspark.ml.feature\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF,StringIndexer,Word2Vec,HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,GBTClassifier,DecisionTreeClassifier\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "spark = SparkSession.builder.appName('text_classification').getOrCreate()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 数据读取\n",
    "try:\n",
    "    df = spark.read.csv('../data/IMDB.csv', header = True, inferSchema = True)\n",
    "except:\n",
    "    # location on server\n",
    "    df = spark.read.csv('file:///home1/cufe/students/wuyuchong/spark_text_classification/data/IMDB.csv', header = True, inferSchema = True)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# 由于数据为逗号分隔的 csv 格式，在文本列出现混淆\n",
    "# 我们使用 pandas 进行读取后再转换为 spark DataFrame 格式\n",
    "pandasDF = pd.read_csv('../data/IMDB.csv')\n",
    "pandasDF.isnull().sum() # 缺失值检查\n",
    "df = spark.createDataFrame(pandasDF)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.groupBy('sentiment').count().show()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 文本清洁\n",
    "if cleaning == False:\n",
    "    df = df.withColumn(\"clean_text\", df.review)\n",
    "else:\n",
    "    try:\n",
    "        # 在服务器上的分布式模式中，需要使用 --py-files 将 gensim 包传到每个子节点\n",
    "        # 若该过程失败则跳过文本清洁过程\n",
    "        import gensim.parsing.preprocessing as gsp\n",
    "        from gensim import utils\n",
    "        filters = [\n",
    "            gsp.strip_tags,\n",
    "            gsp.strip_punctuation,\n",
    "            gsp.strip_multiple_whitespaces,\n",
    "            gsp.strip_numeric,\n",
    "            gsp.remove_stopwords,\n",
    "            gsp.strip_short,\n",
    "            gsp.stem_text\n",
    "        ]\n",
    "        def clean_text(x):\n",
    "            x = x.lower()\n",
    "            x = utils.to_unicode(x)\n",
    "            for f in filters:\n",
    "                x = f(x)\n",
    "            return x\n",
    "\n",
    "        cleanTextUDF = udf(lambda x: clean_text(x), StringType())\n",
    "        df = df.withColumn(\"clean_text\", cleanTextUDF(col(\"review\")))\n",
    "    except:\n",
    "        df = df.withColumn(\"clean_text\", df.review)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 标签数字转换\n",
    "labelEncoder = StringIndexer(inputCol='sentiment', outputCol='label').fit(df)\n",
    "labelEncoder.transform(df).show(5)\n",
    "df = labelEncoder.transform(df)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 数据集划分\n",
    "(trainDF,testDF) = df.randomSplit((0.7,0.3), seed=1)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 文本特征工程\n",
    "tokenizer = Tokenizer(inputCol='clean_text', outputCol='tokens')\n",
    "add_stopwords = [\"<br />\",\"amp\"]\n",
    "stopwords_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered_tokens').setStopWords(add_stopwords)\n",
    "vectorizer = CountVectorizer(inputCol='filtered_tokens', outputCol='rawFeatures')\n",
    "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='vectorizedFeatures')\n",
    "word2Vec = Word2Vec(vectorSize=50, minCount=2, inputCol=\"filtered_tokens\", outputCol=\"vectorizedFeatures\")\n",
    "if processType == 'word2vec':\n",
    "    pipeline = Pipeline(stages=[tokenizer,stopwords_remover,word2Vec])\n",
    "if processType == 'vectorize-idf':\n",
    "    pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf])\n",
    "if processType == 'tf-idf':\n",
    "    pipeline = Pipeline(stages=[tokenizer,stopwords_remover,hashingTF,idf])\n",
    "preprocessModel = pipeline.fit(trainDF)\n",
    "trainDF = preprocessModel.transform(trainDF)\n",
    "testDF = preprocessModel.transform(testDF)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 训练\n",
    "lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "lr_model = lr.fit(trainDF)\n",
    "prediction = lr_model.transform(testDF)\n",
    "prediction.select(['label', 'prediction']).show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(accuracy)\n",
    "# precision recall .... here \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 预测\n",
    "inputText = spark.createDataFrame([(\"I like this movie\",StringType()),\n",
    "                                   (\"It is so bad\",StringType())],\n",
    "                                  [\"clean_text\"])\n",
    "inputText.show(truncate=False)\n",
    "inputText = preprocessModel.transform(inputText)\n",
    "inputPrediction = lr_model.transform(inputText)\n",
    "inputPrediction.show()\n",
    "inputPrediction.select(['clean_text', 'prediction']).show()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 模型比较\n",
    "def logisticCV(trainDF, testDF):\n",
    "    lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = lr.fit(trainDF)\n",
    "    prediction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of logistic regression: %g' % accuracy)\n",
    "\n",
    "def RandomForest(trainDF, testDF):\n",
    "    rf = RandomForestClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = rf.fit(trainDF)\n",
    "    prdiction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of random forest: %g' % accuracy)\n",
    "\n",
    "def GBT(trainDF, testDF):\n",
    "    gbt = GBTClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = gbt.fit(trainDF)\n",
    "    prdiction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of gbt: %g' % accuracy)\n",
    "\n",
    "def DecisionTree(trainDF, testDF):\n",
    "    dt = DecisionTreeClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = dt.fit(trainDF)\n",
    "    prdiction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of decision tree: %g' % accuracy)\n",
    "\n",
    "logisticCV(trainDF, testDF)\n",
    "RandomForest(trainDF, testDF)\n",
    "GBT(trainDF, testDF)\n",
    "DecisionTree(trainDF, testDF)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------> 模型调参\n",
    "def logisticCV(trainDF, testDF):\n",
    "    lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0, 0.5, 2.0]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .addGrid(lr.maxIter, [50, 100, 200]) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of logistic regression: %g' % accuracy)\n",
    "\n",
    "def RandomForestCV(trainDF, testDF):\n",
    "    rf = RandomForestClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .addGrid(rf.maxBins, [16, 32]) \\\n",
    "        .addGrid(rf.minInfoGain, [0, 0.01]) \\\n",
    "        .addGrid(rf.numTrees, [20, 60]) \\\n",
    "        .addGrid(rf.impurity, ['gini', 'entropy']) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of random forest: %g' % accuracy)\n",
    "\n",
    "def GBTClassifierCV(trainDF, testDF):\n",
    "    gbt = GBTClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxDepth, [5, 10]) \\\n",
    "        .addGrid(gbt.maxBins, [16, 32]) \\\n",
    "        .addGrid(gbt.minInfoGain, [0, 0.01]) \\\n",
    "        .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "        .addGrid(gbt.stepSize, [0.1, 0.2]) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of GBT: %g' % accuracy)\n",
    "\n",
    "def DecisionTreeCV(trainDF, testDF):\n",
    "    dt = DecisionTreeClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[dt])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(dt.maxDepth, [5, 10]) \\\n",
    "        .addGrid(dt.maxBins, [16, 32]) \\\n",
    "        .addGrid(dt.minInfoGain, [0, 0.01]) \\\n",
    "        .addGrid(dt.minWeightFractionPerNode, [0, 0.5]) \\\n",
    "        .addGrid(dt.impurity, ['gini', 'entropy']) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of GBT: %g' % accuracy)\n",
    "\n",
    "logisticCV(trainDF, testDF)\n",
    "RandomForestCV(trainDF, testDF)\n",
    "GBTClassifierCV(trainDF, testDF)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 步骤分析等文字待补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "\n",
    "- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)\n",
    "- [BBERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingERT](https://arxiv.org/abs/1810.04805)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
