{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c2683c",
   "metadata": {
    "id": "IZ6SNYq_tVVC",
    "tags": []
   },
   "source": [
    "# IMDB 文本分类和模型比较\n",
    "\n",
    "**摘要：**我们使用 IMDB 数据集进行文本分类。在文本预处理阶段，我们尝试使用词编码和词向量的方式，在训练阶段，我们构建了 DNN、LSTM、BERT 等多个深度学习模型进行训练，并进行了模型比较，最高达到了 99% 的准确率。最后，为了进一步实现在超大文本集上进行训练，我们使用基于 Spark 的分布式算法在集群服务器上进行训练测试。\n",
    "\n",
    "\n",
    "\n",
    "| 模型         | 计算配置 | 用时 | 准确率 | 可拓展性 |\n",
    "| ----------- | ----------- | ------ | ------ | --- |\n",
    "| tokenize + DNN  |阿里云服务器 Xeon 8 核 CPU 32G 内存| 10  分钟 | 60% | 低-单机 |\n",
    "| Word2Vec + LSTM  |阿里云服务器 Xeon 8 核 CPU 32G 内存| 2 小时| 80% | 低-单机 |\n",
    "| bert - 小型       |阿里云服务器 Xeon 8 核 CPU 32G 内存| 1  小时 | 90% | 低-单机 |\n",
    "| bert - AL        |阿里云服务器 Xeon 8 核 CPU 32G 内存| 1.5  小时 | 90% | 低-单机 |\n",
    "| bert - 标准      |阿里云服务器 Xeon 8 核 CPU 32G 内存| 3  小时 | 92% | 低-单机 |\n",
    "| spark            |中央财经大学大数据高性能分布式集群    | --  分钟 | -% | 高-集群 |\n",
    "\n",
    "> 分布式模型在该小型数据集上没有优势，进行此项的意义在于对大型文本数据集可拓展性的技术储备，仅有在文本量级超过单机可承载上限时，分布式计算才具备意义\n",
    "\n",
    "> 注意：请勿在低配置计算机上运行该笔记本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a64de8",
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "source": [
    "## 环境安装及设置\n",
    "\n",
    "请预先安装以下模块：\n",
    "\n",
    "- tensorflow (2.0以上版本）\n",
    "- tensorflow-text（用于文字预处理）\n",
    "- tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1ee36b",
   "metadata": {
    "collapsed": false,
    "id": "_XgTpm9ZxoN9",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 15:01:40.617373: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-16 15:01:40.617424: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydot\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "from official.nlp import optimization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d559c",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "### 标签处理\n",
    "\n",
    "我们将分类列由类别名称转为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02214fa8",
   "metadata": {
    "collapsed": false,
    "id": "6IwI_2bcIeX8",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------> 加载数据\n",
    "dat = pd.read_csv('./data/IMDB.csv')\n",
    "\n",
    "# ------------------> 标签处理\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(dat['sentiment'])\n",
    "y = encoder.transform(dat['sentiment'])\n",
    "text_labels = encoder.classes_\n",
    "text_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b538b1",
   "metadata": {},
   "source": [
    "### 数据集划分\n",
    "\n",
    "随机划分 20% 的测试集，再从训练集中划分 20% 的验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03d4cea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dat['review'], y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6055dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 数据集格式转换\n",
    "\n",
    "我们将数据集转换为 `tf.data.Dataset` 的格式，使用自动的缓冲区大小，使用 32 的 `batch size`\n",
    "\n",
    "batch size 的大小与模型的收敛速度和随机梯度噪音有关。当 batch size 过小时，在一定的迭代次数下，模型来不及收敛。当 batch size 过大时，一方面容易出现内存紧缺，另一方面模型的泛化能力会变差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83c6c40",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 15:01:43.470445: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-16 15:01:43.470481: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-16 15:01:43.470503: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nlp): /proc/driver/nvidia/version does not exist\n",
      "2022-01-16 15:01:43.470806: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train))\n",
    "raw_val_ds = tf.data.Dataset.from_tensor_slices((X_val.values, y_val))\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices((X_test.values, y_test))\n",
    "\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91c836",
   "metadata": {
    "id": "HGm10A5HRGXp"
   },
   "source": [
    "我们对数据集进行预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac12beed",
   "metadata": {
    "collapsed": false,
    "id": "JuxDkcvVIoev",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b\"This is like something I have NEVER seen before. It had me cracking up the whole time I don't think there was one scene that I didn't laugh through. It is about a girl from the country in South who goes off to a big town for college. At the school she befriends the RA across the hall. When she realizes that he has no family to go to for Thanksgiving she invites him to come home with her. Rabecca and her family and her serious boyfriend all go out to dinner one night and Becca realizes what her boyfriend is about to do...Propose. She urges Cral to do something so he stands up and shouts something like... Sorry mate but you are too late I already asked Becca to marry me a couple of weeks ago back at the school and she said yes. That all turns into Chaos. Please watch this classic it is totally worth it... I swear.\"\n",
      "Label : 1 (positive)\n",
      "Review: b'I managed to see this at the New York International Film Festival in November 2005 with my boyfriend. We were both quite impressed with the complexity of the plot and found it to be emotionally moving. It was very well directed with strong imagery. The visual effects were amazing - especially for a short. It had an original fantasy approach to a very real and serious topic: This film is about a young girl who is visited by a demon offering to help her situation with her abusive father. There is also a surprise twist at the end which caught me off guard. This leans towards the Gothic feel. I would love to see this as a full feature film. -- Carrie'\n",
      "Label : 1 (positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 15:01:43.657364: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(2):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({text_labels[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815328b",
   "metadata": {},
   "source": [
    "# Tokenize + DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd2281",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13562c51",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, \n",
    "                                              char_level=False)\n",
    "tokenize.fit_on_texts(X_train) # fit tokenizer to our training text data\n",
    "x_train_token = tokenize.texts_to_matrix(X_train)\n",
    "x_test_token = tokenize.texts_to_matrix(X_test)\n",
    "y_train_token = y_train\n",
    "y_test_token = y_test\n",
    "print('x_train shape:', x_train_token.shape)\n",
    "print('x_test shape:', x_test_token.shape)\n",
    "print('y_train shape:', y_train_token.shape)\n",
    "print('y_test shape:', y_test_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a7996",
   "metadata": {},
   "source": [
    "## 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d755b",
   "metadata": {},
   "source": [
    "经过多次调参，我们发现增大全连接层输出的维度能有效提升准确率。\n",
    "\n",
    "为了减轻训练过程中的过拟合现象，我们使用 dropout 方法来增强神经元的协同适应能力。我们在输入层和中间层分别加入 50% dropout ，即每个神经元有 50% 的概率被随机剔除。由于输出层是我们所需的结果，不使用 dropout 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867b4d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100 \n",
    "drop_ratio = 0.5\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, input_shape=(max_words,)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(drop_ratio))\n",
    "model.add(tf.keras.layers.Dense(512))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(drop_ratio))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee6857",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "正常情况下，随着训练迭代次数的增加，损失函数逐渐减小，对训练集的拟合越来越趋向于精细。然而过度精细的拟合容易导致模型的泛化能力变差，即当模型用于之前未曾训练过的数据时表现很差。为了观测这种情况，我们需要划分一部分数据与用于训练的数据隔开，这便是我们划分验证集的原因之一。\n",
    "\n",
    "为了防止模型过拟合，我们设定在验证集准确率连续三次迭代不再上升时提前终止训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1aaee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "history = model.fit(x_train_token, y_train_token,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback],\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4445ed",
   "metadata": {},
   "source": [
    "## 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74930f71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_token, y_test_token,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dfdfc",
   "metadata": {},
   "source": [
    "# Word2Vec + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0d39e",
   "metadata": {},
   "source": [
    "## 文本预处理\n",
    "\n",
    "我们建立词字典进行词编码，字典的大小限制在 1000 词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b6407",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2cb0cb",
   "metadata": {},
   "source": [
    "编码的长度由所有文本中最长的文本决定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc4e0f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "encoded_example = encoder(text_batch)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef61a9",
   "metadata": {},
   "source": [
    "打印示例："
   ]
  },
  {
   "cell_type": "raw",
   "id": "abd8e695",
   "metadata": {
    "tags": []
   },
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", text_batch[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d0341",
   "metadata": {},
   "source": [
    "## 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7796a",
   "metadata": {},
   "source": [
    "1. 第一层为 `Embedding` 层，我们使用 `word2vec` 方法将单词编码转换为词向量。这些词向量经过训练，对于意思相近的词，其向量夹角小。\n",
    "2. 第二层使用双向的长短期记忆层。长短期记忆网络层是一种特殊的循环神经网络层，它能够减轻长序列训练过程中的梯度消失和梯度爆炸问题，适合此处词向量长度较长的情况。它遍历序列中的每个元素作为输入，按照时间顺序传递输出。由于我们使用双向结构，最终结果由输入的前向和后向传递共同决定，这使得最前端的输入不必通过漫长的处理步数才能影响到最终结果，有效的提高了训练在文本中的均匀度。\n",
    "3. 第三层为全连接层，由于在多层神经网络中梯度容易在深层网络中变得极小，使得参数无法正常更新，所以我们使用 `RELU` 作为激活函数解决梯度消失问题。\n",
    "4. 第四层为输出维度为 5 的输出层，为了得到多分类的概率值，使用 `softmax` 函数将输出值压缩至 0 - 1 的范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3ad1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf40ae9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8919308",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c6547",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=4,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c917c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)\n",
    "plt.savefig('./figure/word2vec_lstm.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742953f7",
   "metadata": {},
   "source": [
    "## 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dda7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c40dc5",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69f17f",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "## BERT 简介\n",
    "\n",
    "BERT 是一系列双向文字编码转换模型的总称，用来结合上下文语义计算每个词的词向量，在自然语言处理中被广泛使用。\n",
    "\n",
    "我们使用了前人在超大型语料库上训练的已有基础 BERT 模型，通过迁移学习的方式在我们的 BBC 文本数据集上进行微调。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b241eb",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "## 加载预训练 BERT 模型\n",
    "\n",
    "我们首先使用了一个参数量较少的 small-BERT 模型用于测试，在通过测试后，为了进一步提升模型的准确度，我们使用 al-BERT 进行正式训练。\n",
    "\n",
    "> 在该笔记本中，我们仅展示 small-BERT 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74da7205",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = 'https://storage.googleapis.com/tfhub-modules/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1.tar.gz'\n",
    "tfhub_handle_preprocess = 'https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_preprocess/3.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c6b0f",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "## 预处理模型\n",
    "\n",
    "在 BERT 的输入层，对于原始的文字输入，我们需要将其转换成为数值编码。每一个 BERT 模型都有其严格对应的预处理模型来提升转换效果。\n",
    "\n",
    "我们展示该预处理模型的输出结果，可以看到该预处理模型将输入的向量设为 128 的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e51d667",
   "metadata": {
    "collapsed": false,
    "id": "r9-zCzJpnuwS",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 1996 2034 6251 1012 1996 2117 6251 1012  102    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "text_test = ['The first sentence. The second sentence.']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e09080",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "## BERT 模型\n",
    "\n",
    "在进行迁移学习之前，我们先看预训练 BERT 模型的输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1951fd1f",
   "metadata": {
    "collapsed": false,
    "id": "_OoF9mebuSZc",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.9992781   0.31525105 -0.25964963  0.55944437  0.00783995  0.9791419\n",
      "  0.9977204  -0.9327053  -0.84368324 -0.98138165 -0.4435     -0.9903451 ]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[ 0.4488559  -0.5851824  -0.6790177  ... -1.764277   -0.45622617\n",
      "   0.35492766]\n",
      " [ 0.24628739  0.08727125 -1.2822778  ... -2.0664744  -0.27477688\n",
      "  -0.2544793 ]\n",
      " [ 0.43256244  0.5654976  -0.79072297 ... -0.65594614 -0.22825454\n",
      "  -0.04055907]\n",
      " ...\n",
      " [ 1.1035298  -0.97563946  0.19795775 ... -1.2058376   0.00869213\n",
      "   0.69881415]\n",
      " [ 0.25988367 -0.39930332 -0.49941957 ... -1.1005459  -0.14869173\n",
      "   0.27366376]\n",
      " [ 0.18883078 -0.47520044 -0.4314221  ... -1.1670263  -0.05709446\n",
      "   0.17906033]]\n"
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0cab11",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## 迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b905f4",
   "metadata": {
    "collapsed": false,
    "id": "aksj743St9ga",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1)(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c1e09",
   "metadata": {
    "id": "Zs4yhFraBuGQ"
   },
   "source": [
    "在开始训练之前，我们测试模型搭建过程是否有误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57a18a8",
   "metadata": {
    "collapsed": false,
    "id": "mGMF8AZcB2Zy",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.57876587]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1cda6f7",
   "metadata": {
    "collapsed": false,
    "id": "0EmzyHZXKIpm",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa98fcdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_type_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'sequence_output':  28763649    ['preprocessing[0][0]',          \n",
      "                                 (None, 128, 512),                'preprocessing[0][1]',          \n",
      "                                 'default': (None,                'preprocessing[0][2]']          \n",
      "                                512),                                                             \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 512),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 512),                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512)]}                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['BERT_encoder[0][5]']           \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,764,162\n",
      "Trainable params: 28,764,161\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca838ec",
   "metadata": {
    "id": "WbUWoZMwc302",
    "tags": []
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e321b5cb",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### 损失函数\n",
    "\n",
    "我们使用交叉熵作为我们的损失函数：\n",
    "\n",
    "$$ -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}) $$\n",
    "\n",
    "其中：\n",
    "\n",
    "- M 是分类数\n",
    "- y 是标签 c 在观测 o 下是否分类正确的 0/1 变量\n",
    "- p 是预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b19059",
   "metadata": {
    "collapsed": false,
    "id": "OWPOZE-L3AgE",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e3d68",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "### 学习率\n",
    "\n",
    "由于神经网络刚开始训练时非常不稳定，因此刚开始的学习率应当设置得很低很低，这样可以保证网络能够具有良好的收敛性。但是较低的学习率会使得训练过程变得非常缓慢，因此这里采用从较低学习率逐渐增大至较高学习率的方式实现网络训练前 10% 次迭代的“热身”阶段。一直使用较高学习率是不合适的，因为它会使得权重的梯度一直来回震荡，很难使训练的损失值达到全局最低谷。因此在 warm-up 结束后，我们使用线性减小的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7876fac",
   "metadata": {
    "collapsed": false,
    "id": "P9eP2y9dbw32",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687eeea6",
   "metadata": {
    "id": "77psrpfzbxtp",
    "tags": []
   },
   "source": [
    "### 优化器\n",
    "\n",
    "在迁移学习时，我们选取的优化器与 BERT 在预训练时的 `Adamw` 优化器保持一致。\n",
    "\n",
    "<img src=\"./figure/Adamw.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Adam 的超收敛性质使其在训练学习率高的神经网络时可以达到节省迭代次数的效果。只要调整得当，Adam 在实践上都能达到 SGD+Momentum 的高准确率，而且速度更快。在几年前人们普遍认为 Adam 的泛化性能不如 SGD+Momentum，然而今年论文表明这通常是由于所选择的超参数不正确导致，通常来说 Adam 需要的正则化比 SGD 更多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344f503",
   "metadata": {
    "collapsed": false,
    "id": "P9eP2y9dbw32",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d690780",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be816bbc",
   "metadata": {
    "collapsed": false,
    "id": "HtfDFAnN_Neu",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "print('----- 训练开始 -----')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)\n",
    "print('----- 训练完成 -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36ecc5",
   "metadata": {
    "collapsed": false,
    "id": "fiythcODf0xo",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('./figure/bert_train.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f3e63",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### 模型评价\n",
    "\n",
    "我们在测试集上计算分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12683b45",
   "metadata": {
    "collapsed": false,
    "id": "slqB-urBV9sP",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fa05e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=22)\n",
    "    plt.yticks(tick_marks, classes, fontsize=22)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=25)\n",
    "    plt.xlabel('Predicted label', fontsize=25)\n",
    "    \n",
    "predict_probability = classifier_model.predict(test_ds)\n",
    "prediction = [np.argmax(i) for i in predict_probability]\n",
    "cnf_matrix = confusion_matrix(y_test.tolist(), prediction)\n",
    "plt.figure(figsize=(24,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=text_labels, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2f1cf",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## 模型应用\n",
    "\n",
    "我们将经过训练完成的模型保存，方便调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d2535",
   "metadata": {
    "collapsed": false,
    "id": "ShcvqJAgVera",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_model.save('./model/IMDB_bert', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d18c7",
   "metadata": {},
   "source": [
    "使用模型对输入的文本进行分类。\n",
    "\n",
    "我们输入一则测试新闻文本：“这部电影很差劲”，该文本被模型分类为消极，符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf72cd2",
   "metadata": {
    "collapsed": false,
    "id": "VBWzH6exlCPS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('./model/IMDB_bert')\n",
    "query = ['This movie is so bad']\n",
    "result = tf.sigmoid(model(tf.constant(query)))\n",
    "print('----- 评论积极的概率 -----')\n",
    "dict(zip(text_labels, result.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf6df3",
   "metadata": {},
   "source": [
    "# 分布式训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7569f2f-e9ae-4af2-92c7-fb7ea17721a8",
   "metadata": {},
   "source": [
    "> 在该笔记本中，我们演示的为并非真实的分布式，而是单机模拟的伪分布式，真实的分布式需要提交到计算机集群。\n",
    "\n",
    "您可在该笔记本中进行代码初步测试，之后通过以下方式提交到中央财经大学高性能大数据计算集群。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "812ab804-f16f-426f-9c6c-dec9d61558d0",
   "metadata": {},
   "source": [
    "ssh cufe@192.168.113.164\n",
    "password: dashuju\n",
    "\n",
    "spark-submit --master yarn code.py --py-files gensim.zip > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126c5de",
   "metadata": {},
   "source": [
    "* 通过 YARN 资源调度系统提交到作业队列： `spark-submit --master yarn`\n",
    "* 由于在 UDF（用户自定义）函数中使用了第三方包，需要将其发送至集群中的每个计算节点 `--py-files gensim.zip`\n",
    "* 队列计算完成后将结果重定向输出 `> output.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d92326-0413-41ea-89ae-8513ae399722",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 环境启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f143536e-ff34-46fa-9ab5-d9587af2aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/16 12:30:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.ml.feature\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF,StringIndexer,Word2Vec,HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,GBTClassifier,DecisionTreeClassifier\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "spark = SparkSession.builder.appName('text_classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920eb6e-49a3-48b5-b5c1-97bf47eb54e1",
   "metadata": {},
   "source": [
    "## 数据读取\n",
    "\n",
    "由于数据为逗号分隔的 csv 格式，在文本列出现混淆。我们使用 pandas 进行读取后再转换为 spark DataFrame 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ec2720-a1c1-4c0d-9756-70cfb2e1b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              review|           sentiment|\n",
      "+--------------------+--------------------+\n",
      "|One of the other ...|            positive|\n",
      "|\"A wonderful litt...| not only is it w...|\n",
      "|\"I thought this w...| but spirited you...|\n",
      "|Basically there's...|            negative|\n",
      "|\"Petter Mattei's ...| power and succes...|\n",
      "|\"Probably my all-...| but that only ma...|\n",
      "|I sure would like...|            positive|\n",
      "|This show was an ...|            negative|\n",
      "|Encouraged by the...|            negative|\n",
      "|If you like origi...|            positive|\n",
      "|\"Phil the Alien i...|            negative|\n",
      "|I saw this movie ...|            negative|\n",
      "|\"So im not a big ...| meaning most of ...|\n",
      "|The cast played S...|            negative|\n",
      "|This a fantastic ...|            positive|\n",
      "|Kind of drawn in ...|            negative|\n",
      "|Some films just s...|            positive|\n",
      "|This movie made i...|            negative|\n",
      "|I remember this f...|            positive|\n",
      "|An awful film! It...|            negative|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:30:48 WARN TaskSetManager: Stage 3 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "|Petter Mattei's \"...| positive|\n",
      "|Probably my all-t...| positive|\n",
      "|I sure would like...| positive|\n",
      "|This show was an ...| negative|\n",
      "|Encouraged by the...| negative|\n",
      "|If you like origi...| positive|\n",
      "|Phil the Alien is...| negative|\n",
      "|I saw this movie ...| negative|\n",
      "|So im not a big f...| negative|\n",
      "|The cast played S...| negative|\n",
      "|This a fantastic ...| positive|\n",
      "|Kind of drawn in ...| negative|\n",
      "|Some films just s...| positive|\n",
      "|This movie made i...| negative|\n",
      "|I remember this f...| positive|\n",
      "|An awful film! It...| negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:30:49 WARN TaskSetManager: Stage 4 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "| positive|25000|\n",
      "| negative|25000|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read.csv('./data/IMDB.csv', header = True, inferSchema = True)\n",
    "except 'FileNotFoundError':\n",
    "    # location on server\n",
    "    df = spark.read.csv('file:///home1/cufe/students/wuyuchong/spark_text_classification/data/IMDB.csv', header = True, inferSchema = True)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "pandasDF = pd.read_csv('./data/IMDB.csv')\n",
    "pandasDF.isnull().sum() # 缺失值检查\n",
    "df = spark.createDataFrame(pandasDF)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.groupBy('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766a139-2492-437f-bf01-750f8a567357",
   "metadata": {},
   "source": [
    "## 文本清洁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416cdc02-958c-4fb6-9db0-8437c1796417",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning = True\n",
    "if cleaning == False:\n",
    "    df = df.withColumn(\"clean_text\", df.review)\n",
    "else:\n",
    "    try:\n",
    "        # 在服务器上的分布式模式中，需要使用 --py-files 将 gensim 包传到每个子节点\n",
    "        # 若该过程失败则跳过文本清洁过程\n",
    "        import gensim.parsing.preprocessing as gsp\n",
    "        from gensim import utils\n",
    "        filters = [\n",
    "            gsp.strip_tags,\n",
    "            gsp.strip_punctuation,\n",
    "            gsp.strip_multiple_whitespaces,\n",
    "            gsp.strip_numeric,\n",
    "            gsp.remove_stopwords,\n",
    "            gsp.strip_short,\n",
    "            gsp.stem_text\n",
    "        ]\n",
    "        def clean_text(x):\n",
    "            x = x.lower()\n",
    "            x = utils.to_unicode(x)\n",
    "            for f in filters:\n",
    "                x = f(x)\n",
    "            return x\n",
    "\n",
    "        cleanTextUDF = udf(lambda x: clean_text(x), StringType())\n",
    "        df = df.withColumn(\"clean_text\", cleanTextUDF(col(\"review\")))\n",
    "    except:\n",
    "        df = df.withColumn(\"clean_text\", df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91396097-95ce-48b9-9716-6731f42ef0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:30:51 WARN TaskSetManager: Stage 7 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:30:52 WARN TaskSetManager: Stage 10 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+\n",
      "|              review|sentiment|          clean_text|label|\n",
      "+--------------------+---------+--------------------+-----+\n",
      "|One of the other ...| positive|review mention wa...|  1.0|\n",
      "|A wonderful littl...| positive|wonder littl prod...|  1.0|\n",
      "|I thought this wa...| positive|thought wonder wa...|  1.0|\n",
      "|Basically there's...| negative|basic famili litt...|  0.0|\n",
      "|Petter Mattei's \"...| positive|petter mattei lov...|  1.0|\n",
      "+--------------------+---------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----------------------------> 标签数字转换\n",
    "labelEncoder = StringIndexer(inputCol='sentiment', outputCol='label').fit(df)\n",
    "labelEncoder.transform(df).show(5)\n",
    "df = labelEncoder.transform(df)\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76d127-b40c-45c3-80d2-0bc32f2696e8",
   "metadata": {},
   "source": [
    "## 数据集划分\n",
    "\n",
    "划分 70% 的训练集和 30% 的测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000263b9-df93-415d-8494-22ee2c8bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF,testDF) = df.randomSplit((0.7,0.3), seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28943c69-2c54-4b4e-a0f5-b96b58a7634f",
   "metadata": {},
   "source": [
    "## 文本特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8633d5-3434-4d6d-a907-dace932d3ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:30:53 WARN TaskSetManager: Stage 11 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:04 WARN TaskSetManager: Stage 15 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:12 WARN DAGScheduler: Broadcasting large task binary with size 1622.5 KiB\n",
      "22/01/16 12:31:12 WARN TaskSetManager: Stage 17 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|              review|sentiment|          clean_text|label|              tokens|     filtered_tokens|         rawFeatures|  vectorizedFeatures|\n",
      "+--------------------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|\" Så som i himmel...| positive|som himmelen spec...|  1.0|[som, himmelen, s...|[som, himmelen, s...|(61505,[1,5,7,8,1...|(61505,[1,5,7,8,1...|\n",
      "|\"A Thief in the N...| positive|thief night film ...|  1.0|[thief, night, fi...|[thief, night, fi...|(61505,[0,1,7,20,...|(61505,[0,1,7,20,...|\n",
      "|\"A bored televisi...| negative|bore televis dire...|  0.0|[bore, televis, d...|[bore, televis, d...|(61505,[0,4,7,14,...|(61505,[0,4,7,14,...|\n",
      "|\"A death at a col...| negative|death colleg camp...|  0.0|[death, colleg, c...|[death, colleg, c...|(61505,[0,6,8,15,...|(61505,[0,6,8,15,...|\n",
      "|\"A wrong-doer is ...| positive|wrong doer man le...|  1.0|[wrong, doer, man...|[wrong, doer, man...|(61505,[1,2,4,6,9...|(61505,[1,2,4,6,9...|\n",
      "|\"ASTONISHING\" Scr...| negative|astonish scream t...|  0.0|[astonish, scream...|[astonish, scream...|(61505,[1,2,3,6,9...|(61505,[1,2,3,6,9...|\n",
      "|\"Ah Ritchie's mad...| positive|ritchi gangster f...|  1.0|[ritchi, gangster...|[ritchi, gangster...|(61505,[1,6,13,34...|(61505,[1,6,13,34...|\n",
      "|\"All men are guil...| positive|men guilti sai ch...|  1.0|[men, guilti, sai...|[men, guilti, sai...|(61505,[0,1,2,3,4...|(61505,[0,1,2,3,4...|\n",
      "|\"Ally McBeal\" was...| negative|alli mcbeal decen...|  0.0|[alli, mcbeal, de...|[alli, mcbeal, de...|(61505,[3,5,16,52...|(61505,[3,5,16,52...|\n",
      "|\"American Nightma...| negative|american nightmar...|  0.0|[american, nightm...|[american, nightm...|(61505,[0,3,10,11...|(61505,[0,3,10,11...|\n",
      "|\"And the time cam...| positive|time came risk re...|  1.0|[time, came, risk...|[time, came, risk...|(61505,[1,3,4,7,9...|(61505,[1,3,4,7,9...|\n",
      "|\"Bell Book and Ca...| positive|bell book candl s...|  1.0|[bell, book, cand...|[bell, book, cand...|(61505,[0,1,4,6,9...|(61505,[0,1,4,6,9...|\n",
      "|\"Bye Bye Birdie\" ...| positive|bye bye birdi isn...|  1.0|[bye, bye, birdi,...|[bye, bye, birdi,...|(61505,[1,3,9,13,...|(61505,[1,3,9,13,...|\n",
      "|\"Checking Out\" is...| positive|check witti hones...|  1.0|[check, witti, ho...|[check, witti, ho...|(61505,[0,1,6,13,...|(61505,[0,1,6,13,...|\n",
      "|\"Cinderella\" is o...| positive|cinderella belov ...|  1.0|[cinderella, belo...|[cinderella, belo...|(61505,[1,3,4,5,7...|(61505,[1,3,4,5,7...|\n",
      "|\"Coconut Fred's F...| positive|coconut fred frui...|  1.0|[coconut, fred, f...|[coconut, fred, f...|(61505,[3,4,5,13,...|(61505,[3,4,5,13,...|\n",
      "|\"Crossfire\" is a ...| positive|crossfir justifi ...|  1.0|[crossfir, justif...|[crossfir, justif...|(61505,[1,4,17,20...|(61505,[1,4,17,20...|\n",
      "|\"Crossfire\" is os...| positive|crossfir ostens m...|  1.0|[crossfir, ostens...|[crossfir, ostens...|(61505,[0,7,9,13,...|(61505,[0,7,9,13,...|\n",
      "|\"Crossfire\" is re...| positive|crossfir rememb f...|  1.0|[crossfir, rememb...|[crossfir, rememb...|(61505,[0,1,4,5,7...|(61505,[0,1,4,5,7...|\n",
      "|\"Cut\" is a full-t...| positive|cut tilt spoof sl...|  1.0|[cut, tilt, spoof...|[cut, tilt, spoof...|(61505,[1,3,4,9,1...|(61505,[1,3,4,9,1...|\n",
      "+--------------------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----------------------------> 特征工程方法选项\n",
    "#  processType = 'word2vec'\n",
    "processType = 'vectorize-idf'\n",
    "#  processType = 'tf-idf'\n",
    "\n",
    "# ----------------------------> 文本特征工程\n",
    "tokenizer = Tokenizer(inputCol='clean_text', outputCol='tokens')\n",
    "add_stopwords = [\"<br />\",\"amp\"]\n",
    "stopwords_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered_tokens').setStopWords(add_stopwords)\n",
    "vectorizer = CountVectorizer(inputCol='filtered_tokens', outputCol='rawFeatures')\n",
    "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='vectorizedFeatures')\n",
    "word2Vec = Word2Vec(vectorSize=50, minCount=2, inputCol=\"filtered_tokens\", outputCol=\"vectorizedFeatures\")\n",
    "if processType == 'word2vec':\n",
    "    pipeline = Pipeline(stages=[tokenizer,stopwords_remover,word2Vec])\n",
    "if processType == 'vectorize-idf':\n",
    "    pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf])\n",
    "if processType == 'tf-idf':\n",
    "    pipeline = Pipeline(stages=[tokenizer,stopwords_remover,hashingTF,idf])\n",
    "preprocessModel = pipeline.fit(trainDF)\n",
    "trainDF = preprocessModel.transform(trainDF)\n",
    "testDF = preprocessModel.transform(testDF)\n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015b3b6-a339-42fd-a612-4cec4fa8aec3",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "我们首先使用简单的 logistic 模型进行拟合，在训练集上进行拟合，之后在测试集上验证模型的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ed2e3b-8fc8-4242-a589-7ee1f030274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:31:17 WARN DAGScheduler: Broadcasting large task binary with size 1624.4 KiB\n",
      "22/01/16 12:31:17 WARN TaskSetManager: Stage 18 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:27 WARN DAGScheduler: Broadcasting large task binary with size 1625.6 KiB\n",
      "22/01/16 12:31:27 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:27 WARN TaskSetManager: Stage 20 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:37 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:37 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/01/16 12:31:37 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/01/16 12:31:37 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:37 WARN TaskSetManager: Stage 22 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:38 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:38 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:38 WARN TaskSetManager: Stage 24 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:38 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:38 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:38 WARN TaskSetManager: Stage 26 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:38 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:38 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:38 WARN TaskSetManager: Stage 28 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:39 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:39 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:39 WARN TaskSetManager: Stage 30 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:39 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:39 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:39 WARN TaskSetManager: Stage 32 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:39 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:40 WARN TaskSetManager: Stage 34 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:40 WARN TaskSetManager: Stage 36 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:40 WARN TaskSetManager: Stage 38 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:40 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:40 WARN TaskSetManager: Stage 40 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:41 WARN TaskSetManager: Stage 42 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:41 WARN TaskSetManager: Stage 44 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:42 WARN TaskSetManager: Stage 46 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:42 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:42 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:42 WARN TaskSetManager: Stage 48 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:42 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:42 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:42 WARN TaskSetManager: Stage 50 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:43 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:43 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:43 WARN TaskSetManager: Stage 52 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:43 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:43 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:43 WARN TaskSetManager: Stage 54 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:44 WARN TaskSetManager: Stage 56 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:44 WARN TaskSetManager: Stage 58 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:44 WARN TaskSetManager: Stage 60 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:45 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:45 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:45 WARN TaskSetManager: Stage 62 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:45 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:45 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:45 WARN TaskSetManager: Stage 64 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:45 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:46 WARN TaskSetManager: Stage 66 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:46 WARN TaskSetManager: Stage 68 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:46 WARN TaskSetManager: Stage 70 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:47 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:47 WARN TaskSetManager: Stage 72 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:47 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:47 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:47 WARN TaskSetManager: Stage 74 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:47 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:47 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:47 WARN TaskSetManager: Stage 76 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:48 WARN TaskSetManager: Stage 78 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:48 WARN TaskSetManager: Stage 80 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:48 WARN TaskSetManager: Stage 82 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:49 WARN TaskSetManager: Stage 84 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:49 WARN TaskSetManager: Stage 86 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:49 WARN TaskSetManager: Stage 88 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:50 WARN TaskSetManager: Stage 90 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:50 WARN TaskSetManager: Stage 92 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:50 WARN TaskSetManager: Stage 94 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:51 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:51 WARN TaskSetManager: Stage 96 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:51 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:51 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "22/01/16 12:31:51 WARN TaskSetManager: Stage 98 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:51 WARN DAGScheduler: Broadcasting large task binary with size 1627.0 KiB\n",
      "22/01/16 12:31:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/01/16 12:31:52 WARN TaskSetManager: Stage 100 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/01/16 12:31:57 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/01/16 12:31:57 WARN TaskSetManager: Stage 101 contains a task of very large size (8022 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8366346605787954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "lr_model = lr.fit(trainDF)\n",
    "prediction = lr_model.transform(testDF)\n",
    "prediction.select(['label', 'prediction']).show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236e4ba-7bc1-4206-b98a-3f3644341620",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a925494-e55e-43df-87cb-0e329ebbf09d",
   "metadata": {},
   "source": [
    "我们准备了两个测试用例来验证模型是否有效。\n",
    "\n",
    "1. 我喜欢这部电影\n",
    "2. 它很差劲\n",
    "\n",
    "模型对前一个句子的分类结果为积极，对一个句子的分类结果为消极。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a81056a-71c9-4658-bfa9-4b4c3aa9d66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|clean_text       |_2 |\n",
      "+-----------------+---+\n",
      "|I like this movie|{} |\n",
      "|It is so bad     |{} |\n",
      "+-----------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:32:04 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/01/16 12:32:04 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/01/16 12:32:05 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       clean_text| _2|              tokens|     filtered_tokens|         rawFeatures|  vectorizedFeatures|       rawPrediction|         probability|prediction|\n",
      "+-----------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|I like this movie| {}|[i, like, this, m...|[i, like, this, m...|(61505,[2,11578,4...|(61505,[2,11578,4...|[-15.575416022470...|[1.72061099333001...|       1.0|\n",
      "|     It is so bad| {}|   [it, is, so, bad]|   [it, is, so, bad]|(61505,[11,15663,...|(61505,[11,15663,...|[4.04521599411133...|[0.98279526175309...|       0.0|\n",
      "+-----------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+-----------------+----------+\n",
      "|       clean_text|prediction|\n",
      "+-----------------+----------+\n",
      "|I like this movie|       1.0|\n",
      "|     It is so bad|       0.0|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 12:32:05 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/01/16 12:32:05 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/01/16 12:32:05 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    }
   ],
   "source": [
    "inputText = spark.createDataFrame([(\"I like this movie\",StringType()),\n",
    "                                   (\"It is so bad\",StringType())],\n",
    "                                  [\"clean_text\"])\n",
    "inputText.show(truncate=False)\n",
    "inputText = preprocessModel.transform(inputText)\n",
    "inputPrediction = lr_model.transform(inputText)\n",
    "inputPrediction.show()\n",
    "inputPrediction.select(['clean_text', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df814c6-3198-4e23-b561-53fbf9523d1f",
   "metadata": {},
   "source": [
    "## 模型比较\n",
    "\n",
    "在 logistic 模型的基础上，我们还搭建了随机森林模型、梯度助推树模型、决策树模型。\n",
    "\n",
    "> 此处对计算性能要求较高，请提交至集群进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd949e5f-5f4e-4b8f-8a7a-164e79f49a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticCV(trainDF, testDF):\n",
    "    lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = lr.fit(trainDF)\n",
    "    prediction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of logistic regression: %g' % accuracy)\n",
    "\n",
    "def RandomForest(trainDF, testDF):\n",
    "    rf = RandomForestClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = rf.fit(trainDF)\n",
    "    prdiction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of random forest: %g' % accuracy)\n",
    "\n",
    "def GBT(trainDF, testDF):\n",
    "    gbt = GBTClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = gbt.fit(trainDF)\n",
    "    prdiction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of gbt: %g' % accuracy)\n",
    "\n",
    "def DecisionTree(trainDF, testDF):\n",
    "    dt = DecisionTreeClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    model = dt.fit(trainDF)\n",
    "    prdiction = model.transform(testDF)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy of decision tree: %g' % accuracy)\n",
    "\n",
    "logisticCV(trainDF, testDF)\n",
    "RandomForest(trainDF, testDF)\n",
    "GBT(trainDF, testDF)\n",
    "DecisionTree(trainDF, testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d25775-c416-442a-91a4-c7e116491c93",
   "metadata": {},
   "source": [
    "## 模型调参\n",
    "\n",
    "我们使用网格搜索的方式对几个模型的超参数进行调整，选取最优的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f56213-6db1-4ec7-8acb-8b179dea5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticCV(trainDF, testDF):\n",
    "    lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0, 0.5, 2.0]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .addGrid(lr.maxIter, [50, 100, 200]) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of logistic regression: %g' % accuracy)\n",
    "\n",
    "def RandomForestCV(trainDF, testDF):\n",
    "    rf = RandomForestClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .addGrid(rf.maxBins, [16, 32]) \\\n",
    "        .addGrid(rf.minInfoGain, [0, 0.01]) \\\n",
    "        .addGrid(rf.numTrees, [20, 60]) \\\n",
    "        .addGrid(rf.impurity, ['gini', 'entropy']) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of random forest: %g' % accuracy)\n",
    "\n",
    "def GBTClassifierCV(trainDF, testDF):\n",
    "    gbt = GBTClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxDepth, [5, 10]) \\\n",
    "        .addGrid(gbt.maxBins, [16, 32]) \\\n",
    "        .addGrid(gbt.minInfoGain, [0, 0.01]) \\\n",
    "        .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "        .addGrid(gbt.stepSize, [0.1, 0.2]) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of GBT: %g' % accuracy)\n",
    "\n",
    "def DecisionTreeCV(trainDF, testDF):\n",
    "    dt = DecisionTreeClassifier(featuresCol='vectorizedFeatures',labelCol='label')\n",
    "    pipeline = Pipeline(stages=[dt])\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(dt.maxDepth, [5, 10]) \\\n",
    "        .addGrid(dt.maxBins, [16, 32]) \\\n",
    "        .addGrid(dt.minInfoGain, [0, 0.01]) \\\n",
    "        .addGrid(dt.minWeightFractionPerNode, [0, 0.5]) \\\n",
    "        .addGrid(dt.impurity, ['gini', 'entropy']) \\\n",
    "        .build() \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "    crossValidator = CrossValidator(estimator=pipeline, \n",
    "                                    evaluator=evaluator,\n",
    "                                    estimatorParamMaps=paramGrid,\n",
    "                                    numFolds=5)\n",
    "    cv = crossValidator.fit(trainDF)\n",
    "    best_model = cv.bestModel.stages[0]\n",
    "    prediction = best_model.transform(testDF)\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    print('Accuracy in Cross Validation of GBT: %g' % accuracy)\n",
    "\n",
    "logisticCV(trainDF, testDF)\n",
    "RandomForestCV(trainDF, testDF)\n",
    "GBTClassifierCV(trainDF, testDF)\n",
    "DecisionTreeCV(trainDF, testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891235e",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "\n",
    "- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)\n",
    "- [BBERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingERT](https://arxiv.org/abs/1810.04805)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
