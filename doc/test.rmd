---
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    template: template.tex
    highlight: espresso
classoption: "hyperref,"
geometry: margin=1in
csl: chinese-gb7714-2005-numeric.csl
# bibliography: reference.bib
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{indentfirst}
   - \setlength{\parindent}{4em}
logo: "cufe.jpg"
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.pos = 'H', echo = TRUE, warning = FALSE, message = FALSE, comment = NA)
library(rmarkdown)
library(knitr)
library(tidyverse)
library(dplyr)
# base_family = 'STXihei'
```

\tableofcontents

\newpage

# 摘要

我们使用 IMDB 数据集进行文本分类。在文本预处理阶段，我们尝试使用词编码和词向量的方式，在训练阶段，我们构建了 DNN、LSTM、BERT 等多个深度学习模型进行训练，并进行了模型比较，最高达到了 99% 的准确率。最后，为了进一步实现在超大文本集上进行训练，我们使用基于 Spark 的分布式算法在集群服务器上进行训练测试。

| 模型         | 计算配置    | 用时   | 准确率 | 可拓展性 |
| ------------ | ----------- | ------ | ------ | -------- |
| tokenize + DNN   |阿里云服务器 Xeon 8 核 CPU 32G 内存| 10 分钟 | 60% | 低-单机 |
| Word2Vec + LSTM  |阿里云服务器 Xeon 8 核 CPU 32G 内存| 2  小时 | 80% | 低-单机 |
| bert - 小型      |阿里云服务器 Xeon 8 核 CPU 32G 内存| 1  小时 | 90% | 低-单机 |
| bert - AL        |阿里云服务器 Xeon 8 核 CPU 32G 内存| 1.5小时 | 90% | 低-单机 |
| bert - 标准      |阿里云服务器 Xeon 8 核 CPU 32G 内存| 3  小时 | 92% | 低-单机 |
| spark            |中央财经大学大数据高性能分布式集群 | -- 分钟 | --% | 高-集群 |

> 分布式模型在该小型数据集上没有优势，进行此项的意义在于对大型文本数据集可拓展性的技术储备，仅有在文本量级超过单机可承载上限时，分布式计算才具备意义

# 数据集介绍

我们选择了 IMDB 的电影评论文本数据进行大数据建模研究。

IMDB 是一个隶属于亚马逊公司旗下的世界著名互联网电影资料库（Internet Movie Database）。它有着关于电影演员、电影、电视节目、电视明星和电影制作的在线数据，包括了影片的众多信息、演员、片长、内容介绍、分级、评论等，在电影评论评分时被广泛使用。IMDB 的论坛也十分活跃，除每个数据库条目都有留言板之外，还有关于多种多样的主题的各种综合讨论版。

我们将 IMDB 的电影评论文本用于自然语言处理的二元情感分类。我们使用 5 万条标有积极和消极标签的真实用户电影评论文本构建情感分类模型。 即使用深度学习算法预测评论为正面或是负面。

我们使用的文本为多语言文本，其中英文文本数量占绝大多数比例。

# 方法综述

## 文本预处理

### 分词

分词是NLP（自然语言处理）中的一个重要基础步骤，就是将原始数据中的文章、段落、句子这样的长文本的集合，拆分成以单词作为单位的数据，这样有利于后续的处理工作。NLP同其他机器学习算法一样，通过将不同类型的问题转化成数学问题，来解决众多复杂课题。文本作为非结构化数据，经一定步骤转化为结构化数据，就可以进一步转化为数学问题，从而进行分类、预测、LDA主题模型等处理，而分词就是转化的最初一步，也是重要步骤。另一方面，单词是比较适合于研究的粒度。字或字母的粒度太小，无法传递清晰完整的含义，中文中的字可能有多种含义，而英文中的字母则通常没有具体意义；而句子或段落的粒度过大，包含的信息量大，很难通过算法进行提炼并重复使用。因此，单词作为能够传递完整语义的最小构成单位，自然成为了NLP中使用的粒度。

英文分词与中文相比，英文有空格作为天然的分隔符，因此切分起来相对容易。但另一方面，英文中的单词拥有复杂多变的变换形式，如动词存在时态变化、名词存在单复数不同形式等，若不进行还原提取的话，就会增加数据的稀疏性，不利于后续的建模分析。

### 文本清洗

由于实际操作中使用的原始数据通常并不是完美的格式，如：爬虫获取的数据时常存在html标签，句子或段落中存在的换行符、制表符等非空格空白符，都会对分词造成干扰，增加没有实际意义的单词，影响后续的数据分析，分词前的文本清洗就显得十分有必要。本文使用的数据为爬虫得到的影评，因此分词前文本清洗步骤主要为使用正则表达式re库，去除html标签、去除被和谐词，及常见的去除单引号、去除非空格空白符等。

文本中存在的人称代词、常见动词、名词等出现频率很高，但并不能反映句子或段落的主要含义，这些单词被归为停用词，在文本中被视为噪音。对于分词后的数据，为不影响后续的分析结果，应该将停用词予以删除。本文使用了NLTK的语料库中的英文停用词列表，并相应增加了一些自定义的单词来进行停用词处理。

### 词形还原

词干提取（Stemming）与词形还原（Lemmatisation）是英文语料预处理过程中分词后的重要环节，他们的目的都是将具有不同派生形态的单词简化或归并为词干或原形，对词的不同形态进行统一归并，从而降低数据的稀疏性及规模，方便后续的处理与分析。二者也存在不同点：

词干提取基于的原理是“缩减“，去除单词的前后缀（名词复数、过去分词、进行式等）得到词根，如plays、played、playing经提取后都得到play。相较而言，词干提取相对简单，但提取得到的结果可能只是词的一部分，并非完整的、有意义的单词。目前有三种常见的词干提取算法： （1）Porter：该算法开始于20世纪80年代，目前已经停止开发。重点放在删除单词的共同结尾，方便解析为通用形式。通常情况下，在研究中它可以作为一种很好的基本词干算法，作为起始的词干分析器，能够保证重复性，同时相较其他算法也比较温和，但并不太适合进行复杂的应用；（2）Snowball：也被成为Porter2算法，在Porter2的基础上进行了许多优化，分词结果差异约在5%左右；（3）Lancaster：该算法比较激进，有时候会把单词处理成一些比较怪异的形式。如果是在NLTK中进行分词，则可以非常轻松地往该算法中添加自定义规则。

而词形还原则的原理则是“转变“，基于字典将单词的复杂形式转变为其原形，如drove、driving都将被处理成drive。因此，词形还原更为复杂，不仅需要转化词缀，还需要区分原形不同但具有相同词性的单词（如leaves可能为动词leave的第三人称形式，也可能为名词leaf的复数形式），根据上下文进行词性识别。与词干提取相比，词形还原后的单词完整、具有一定意义，更具有研究与应用的价值。

本文使用gensim库的models包中的Phrases与phrases.Phraser模块训练bigram与trigram模型，通过前后单词辅助词性识别；使用spacy库中的英文字典进行词形还原。

### 分类算法

在本研究中，我们使用了一些业界常见的算法进行分类。

（1）	朴素贝叶斯

该算法是一种典型的概率模型算法，通过数据与先验概率决定后验概率。优点是原理简单、分类的性能稳定，待估参数的数量少，对缺失值不敏感，适合进行增量式训练。不足之处在于容易因为先验模型假设的问题，造成预测效果不好，同时当属性之间关联性较强，或者数量较多时，分类结果较差。

（2）	K最近邻（KNN）

主要原理是待分类样本的类别，由特征空间中离它最近的样本类别来决定。该算法可以适用于样本量较大的文本数据集，其优势在于训练的代价较低，对类域交叉或重叠较多的样本比较容易处理。其缺点是它的时空复杂度较大，当数据集存在偏斜，或样本量较小时容易产生误分类，同时初始k值的选择会影响分类效果。

（3）	支持向量机（SVM）

本算法通过学习来寻找间隔最大的超平面，来对样本数据进行分割。在处理高维稀疏、小样本数据时有优势，并且可以解决非线性问题。但它的训练速度比较慢，对数据缺失很敏感，同时核函数的选择缺少统一的标准。

（4）	决策树

主要原理是在已知各种情况发生的概率的基础上，将数据所有特征的判断级连起来，通过一系列判断规则来对样本进行分类。优点是易于理解与解释，计算复杂度不高。但当数据量超过一定程度时，规则库会过大，容易因数据敏感性增强产生过拟合。与其他文本分类算法相比性能较弱。

（5）	Rocchio算法

该算法使用训练语料来为每个类别构造一个质心向量，通过计算待分类文档与质心向量的相似性来划分类别，相似性的度量通常采用夹角余弦、向量内积、欧式距离等。它的算法简单，易于实现与理解，训练与分类的效率很高。但容易受样本分布影响，且构造的质心向量可能会落在所属的类别之外。1][2]

（6）	深度学习分类算法

与以上传统的文本分类算法相比，深度学习作为机器学习的重要分支，可以通过多层语义操作，得到更加抽象、层级更多的语义特征。它的优势还包括能够在模型的建立过程中融入特征提取工作，使得人为设计特征导致的不完备性与冗余得到减少。常用于文本分类的深度学习算法主要有：卷积神经网络（CNN）、循环神经网络（RNN）、注意力机制（Attention Mechanism）等及以上算法的一些结合。[3]

### 主题模型

王博等人使用LDA主题模型应用于专利内容分析领域，划分不同专利的主题，并在原有模型上建立LDA机构-主题模型，将专利的主体和客体相结合联合建模，实现内在关系分析。（王博等，2015）关鹏等使用LDA主题模型从定性和定量两个方面对于关键词、摘要以及关键词和摘要共同构建文本语料库的不同方式进行评价。（关鹏等，2016）黄佳佳等对主题模型进行了系统性研究，分析归纳总结了传统主题模型各类型的联系区别和优势，并综述了基于深度学习的主题模型研究，分析并展望了其未来发展趋势。（黄佳佳等，2019）胡吉明等将增量Gibbs抽样估计算法引入LDA主题模型，进行了动态化改进，将主题的内容和强度进行了分析，将文本时间信息纳入模型。（胡吉明等，2014）陈晓美等在网络舆论使用LDA主题模型提取舆情观点，从海量舆论评论中摘取深度评论的主要观点，拓展用户认知深度和广度（陈晓美等，2015）

主题模型能够自动将文本语料库编码成一组具有实质性意义的类别，这些类别称之为主题。主题模型分析的典型代表是隐含迪利克雷分布也就是LDA，LDA最明显的特征是能够将若干文档自动编码分类为一定数量的主题，极大减少了认为干预及负担。主题数量需要人为指定，在这里我们将主题数量定为4，该模型可以得到每个主题下词语的分布概率。LDA的工作原理是设定α和β两个参数决定模型的工作状态，得到一篇随机生成的文档，通过对比这篇文档与原文档的相似性可以判断该参数下模型的好坏，在不同参数的很多模型中，最终找到最优的模型，即找到最佳的α与β的值。α和β分别控制一个迪利克雷分布，α随机生成文档对应主题的多项式分布θ，θ随机生成一个主题z，而β随机生成主题对应词语的多项式分布φ，综合主题z和主义对应词语分布情况φ生成词语w，如此循环生成一个文档，包含m个词语，最终生成k个主题下的n篇文档。

将预处理后的数据集放入一个二维列表，将列表里的单词去掉停止词后，使用gensim库提供的函数直接提取成一部无重复单词的字典，接着统计训练语料的词频，运用LDA模型进行训练，得到四个主题下词语的分布概率。参数corpus用于确定语料库；参数id2word用于确定词汇表大小，以及调试和主题打印；参数passes确定训练期间通过语料库的次数；参数num_topics确定主题的数量；参数update_every 确定每次更新要迭代的文档数，在此迭代学习设置为1。下图为最后得到的各主体关键词的分布概率。

文字云是关键词的视觉化描述，把需要的关键词绘制成图片，通过改变字体大小或颜色来表现其重要程度。这里在每个主题下展示了十个词频最高的单词，其频率大小以字体大小来体现。参数max_words指定词云中能显示的最多单词数；参数stop_words指定在词云中不显示的单词列表；background_color指定词云图片的背景颜色。

## 文本预处理

# 主题挖掘

# 深度学习

## 数据处理

1. 标签处理：分类标签由类别名称转为数字。
1. 数据集划分：在总体 5 万条文本中随机划分 20% 的测试集，再从训练集中划分 20% 的验证集。
    * 使用训练集的文本进行模型训练
    * 使用验证集的文本进行模型超参数的调整
    * 使用测试集的文本进行模型效果评价
1. 数据集格式转换：使用自动的缓冲区大小，使用 32 的 `batch size`。
    * batch size 为一次训练所抓取的数据样本数量
    * 分批训练相对于直接对全训练集训练的好处在于：提高了每次迭代的训练速度、利于多线程训练、使得梯度下降的方向更加准确
    * batch size 的大小与模型的收敛速度和随机梯度噪音有关
    * 当 batch size 过小时，在一定的迭代次数下，模型来不及收敛
    * 当 batch size 过大时，一方面容易出现内存紧缺，另一方面模型的泛化能力会变差

## Tokenize + DNN

### 文本预处理

### DNN 模型结构

我们搭建了一个三层神经网络用于训练。

```{r}
layer = c('输入层', 'drop out', '中间层', 'drop out', '输出层')
unit = c(512, 0, 512, 0, 1)
para = c(512512, 0, 262656, 0, 513)
table = data.frame(layer, unit, para)
kable(table, caption = '搭建的神经网络结构（总参数个数：775681）')
```
